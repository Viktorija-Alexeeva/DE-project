{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c07b2b3-4d3b-4cf3-bb60-eeab71873f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed709211-644d-4980-8183-96fbe99943bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "credentials_location = '/home/viktorija/.gc/prices.json'\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"./lib/gcs-connector-hadoop3-2.2.5.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff2e295-8522-47f8-a81e-229926e1f062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/23 10:38:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90510991-3c75-4e22-9699-aa16c4d0d6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "93d10fa5-06ff-42dc-824b-241c233087ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "eb00b9d8-b4dd-4221-987d-d299b6bf7c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "778eaa37-b818-403b-91dc-2cc3130e7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_schema = types.StructType([\n",
    "    types.StructField(\"id\", types.IntegerType(), True),\n",
    "    types.StructField(\"name\", types.StringType(), True),\n",
    "    types.StructField(\"host_id\", types.IntegerType(), True),\n",
    "    types.StructField(\"host_name\", types.StringType(), True),\n",
    "    types.StructField(\"neighbourhood_group\", types.StringType(), True),\n",
    "    types.StructField(\"neighbourhood\", types.StringType(), True),\n",
    "    types.StructField(\"latitude\", types.StringType(), True),\n",
    "    types.StructField(\"longitude\", types.StringType(), True),\n",
    "    types.StructField(\"room_type\", types.StringType(), True),\n",
    "    types.StructField(\"price\", types.DoubleType(), True),\n",
    "    types.StructField(\"minimum_nights\", types.IntegerType(), True),\n",
    "    types.StructField(\"number_of_reviews\", types.IntegerType(), True),\n",
    "    types.StructField(\"last_review\",types.DateType(), True),  \n",
    "    types.StructField(\"reviews_per_month\", types.IntegerType(), True),\n",
    "    types.StructField(\"calculated_host_listings_count\", types.IntegerType(), True),\n",
    "    types.StructField(\"availability_365\", types.IntegerType(), True),\n",
    "    types.StructField(\"number_of_reviews_ltm\", types.IntegerType(), True),\n",
    "    types.StructField(\"license\", types.StringType(), True),\n",
    "    types.StructField(\"country\", types.StringType(), True),\n",
    "    types.StructField(\"region\", types.StringType(), True),\n",
    "    types.StructField(\"city\", types.StringType(), True),\n",
    "    types.StructField(\"release_date\", types.DateType(), True) \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "357260af-8fc7-4c02-85bd-f009a8505e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import md5, concat_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e2b678e4-9dba-4439-9bc1-d6dd060af8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spain = spark.read\\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(df_schema) \\\n",
    "    .csv('gs://airbnb-prices-eu-bucket/Spain/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "da62a802-4507-4c14-9d69-55de897b4333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- host_id: integer (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- neighbourhood_group: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- number_of_reviews: integer (nullable = true)\n",
      " |-- last_review: date (nullable = true)\n",
      " |-- reviews_per_month: integer (nullable = true)\n",
      " |-- calculated_host_listings_count: integer (nullable = true)\n",
      " |-- availability_365: integer (nullable = true)\n",
      " |-- number_of_reviews_ltm: integer (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- release_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spain.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f7991ffa-a375-4496-851d-bb9d83e1ebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    " df_spain_uni = df_spain.withColumn(\"unique_row_id\", md5(concat_ws(\"|\", \"country\", \"region\", \"city\", \"release_date\", \"id\", \"host_id\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "440111e6-cf3c-47be-b8b3-9fe40cba65c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- host_id: integer (nullable = true)\n",
      " |-- host_name: string (nullable = true)\n",
      " |-- neighbourhood_group: string (nullable = true)\n",
      " |-- neighbourhood: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- room_type: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- minimum_nights: integer (nullable = true)\n",
      " |-- number_of_reviews: integer (nullable = true)\n",
      " |-- last_review: date (nullable = true)\n",
      " |-- reviews_per_month: integer (nullable = true)\n",
      " |-- calculated_host_listings_count: integer (nullable = true)\n",
      " |-- availability_365: integer (nullable = true)\n",
      " |-- number_of_reviews_ltm: integer (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- release_date: date (nullable = true)\n",
      " |-- unique_row_id: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spain_uni.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b3edcb20-ca79-49b0-b39e-51ebee7fb172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=21853, name='Bright and airy room', host_id=83531, host_name='Abdel', neighbourhood_group='Latina', neighbourhood='CÃ¡rmenes', latitude='40.40381', longitude='-3.7413', room_type='Private room', price=29.0, minimum_nights=4, number_of_reviews=33, last_review=datetime.date(2018, 7, 15), reviews_per_month=None, calculated_host_listings_count=2, availability_365=233, number_of_reviews_ltm=0, license=None, country='spain', region='comunidad-de-madrid', city='madrid', release_date=datetime.date(2025, 3, 5), unique_row_id='180dd48d4d8c14fa93c6f14e0025e226'),\n",
       " Row(id=30320, name='Great Vacational Apartments', host_id=130907, host_name='Dana', neighbourhood_group='Centro', neighbourhood='Sol', latitude='40.41476', longitude='-3.70418', room_type='Entire home/apt', price=None, minimum_nights=5, number_of_reviews=172, last_review=datetime.date(2022, 9, 26), reviews_per_month=None, calculated_host_listings_count=3, availability_365=0, number_of_reviews_ltm=0, license=None, country='spain', region='comunidad-de-madrid', city='madrid', release_date=datetime.date(2025, 3, 5), unique_row_id='1f84060acb4e136a718e87a8ebdf2804')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spain_uni.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e73b6436-e570-45a4-96b4-676f5cc44ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spain_uni.createOrReplaceTempView('spain_listings_temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e44344fe-0f18-4e65-822f-5eb2b53b4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spain_bq = spark.sql(\"\"\"\n",
    "SELECT  *    \n",
    "FROM\n",
    "    spain_listings_temp\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c9a661bd-525f-456a-866a-bb5712adcff5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o668.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: bigquery. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m df_spain_bq\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbigquery\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mairbnb_prices_eu_dataset.spain_temp\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemporaryGcsBucket\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataproc-temp-europe-west1-95215738418-sjnvgog3\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/spark/spark-3.5.5-bin-hadoop3/python/pyspark/sql/readwriter.py:1461\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave(path)\n",
      "File \u001b[0;32m~/spark/spark-3.5.5-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/spark/spark-3.5.5-bin-hadoop3/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/spark/spark-3.5.5-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o668.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: bigquery. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:251)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:834)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:588)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:521)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "df_spain_bq.write.format('bigquery') \\\n",
    "    .option('table', 'airbnb_prices_eu_dataset.spain_temp') \\\n",
    "    .option('temporaryGcsBucket', 'dataproc-temp-europe-west1-95215738418-sjnvgog3') \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2cd797-ae75-45a0-9ce0-71929804c4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
