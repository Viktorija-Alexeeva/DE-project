id: upload_data_to_gcs_and_bq
namespace: de-project

inputs:
  - id: country
    type: SELECT
    displayName: Select country
    values: [spain, portugal, italy, greece, france, germany]
    defaults: spain
    allowCustomValue: true
 
tasks:
  - id: extract_and_add_columns
    type: io.kestra.plugin.scripts.python.Script
    outputFiles:
      - "*.csv"
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    warningOnStdErr: false
    beforeCommands:
      - pip install beautifulsoup4
      - pip install pandas
      - pip install unidecode
    script: |
      import os
      import re
      import logging
      import requests
      import pandas as pd
      from bs4 import BeautifulSoup
      from unidecode import unidecode

      # Configure logging
      logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

      BASE_URL = "https://insideairbnb.com/get-the-data/"
      TARGET_COUNTRY = "{{inputs.country}}".lower() 

      # Define text normalization function
      def clean_text(text):
          """Clean and normalize text values from CSV fields."""
          if text:
              text = re.sub(r'[\n\t]', ' ', text)
              text = re.sub(r'^[\'",\s]+|[\'",\s]+$', '', text)
              text = unidecode(text)
              text = re.sub(r'\s+', ' ', text)
              return text
          return text

      try:
          # Fetch and parse the HTML
          logging.info("Fetching data from Inside Airbnb...")
          response = requests.get(BASE_URL)
          response.raise_for_status()
          soup = BeautifulSoup(response.text, 'html.parser')
          all_links = soup.find_all("a")

          # Extract relevant listings.csv links for the target country
          listings_links = [
              link.get("href") for link in all_links
              if link.get("href") 
              and "visualisations/listings.csv" in link.get("href").lower()
              and TARGET_COUNTRY in link.get("href").lower()
          ]

          if not listings_links:
              logging.warning("No matching listings.csv files found for the given country.")

          # Use current working directory for downloads
          download_dir = os.getcwd()

          for link in listings_links:
              try:
                  # Extract metadata from URL parts
                  filename_parts = link.split("/")
                  country, region, city, release_date = filename_parts[3:7]
                  file_name = f"{country}_{region}_{city}_{release_date}_listings.csv"
                  file_path = os.path.join(download_dir, file_name)

                  # Remove existing file if it already exists
                  if os.path.exists(file_path):
                      os.remove(file_path)

                  # Download the file using requests with streaming
                  logging.info(f"Downloading {file_name}...")
                  with requests.get(link, stream=True) as r:
                      r.raise_for_status()
                      with open(file_path, 'wb') as f:
                          for chunk in r.iter_content(chunk_size=8192):
                              f.write(chunk)
                  logging.info(f"Downloaded: {file_name}")

                  # Load into pandas DataFrame
                  df = pd.read_csv(file_path, low_memory=False, encoding='utf-8', on_bad_lines='skip')

                  # Clean specific columns
                  columns_to_clean = [
                      'id', 'name', 'host_id', 'host_name', 'neighbourhood_group', 'neighbourhood',
                      'latitude', 'longitude', 'room_type', 'price', 'minimum_nights', 'number_of_reviews',
                      'last_review', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365',
                      'number_of_reviews_ltm', 'license'
                  ]
                  for col in columns_to_clean:
                      if col in df.columns:
                          df[col] = df[col].apply(lambda x: clean_text(str(x)) if pd.notnull(x) else x)

                  # Add metadata columns
                  df["country"] = country
                  df["region"] = region
                  df["city"] = city
                  df["release_date"] = release_date

                  # Save updated CSV
                  df.to_csv(file_path, index=False)
                  logging.info(f"Updated file with additional columns: {file_name}")

              except Exception as e:
                  logging.error(f"Failed to process {link}: {e}")

      except Exception as e:
          logging.critical(f"Failed to fetch and parse data from {BASE_URL}: {e}")

  - id: create_bq_table
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
        CREATE TABLE IF NOT EXISTS `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.{{inputs.country}}_listings` (
          id INT,
          name STRING,
          host_id INT,
          host_name STRING,
          neighbourhood_group STRING,
          neighbourhood STRING,
          latitude STRING,
          longitude STRING,
          room_type STRING,
          price FLOAT64,
          minimum_nights INT,
          number_of_reviews INT,
          last_review DATE,
          reviews_per_month FLOAT64,
          calculated_host_listings_count INT,
          availability_365 INT,
          number_of_reviews_ltm INT,
          license STRING,
          country STRING,
          region STRING,
          city STRING,
          release_date DATE,
          unique_row_id BYTES 
        )
        PARTITION BY release_date
        CLUSTER BY city
        ;

  - id: for_each
    type: io.kestra.plugin.core.flow.ForEach
    values: "{{ outputs.extract_and_add_columns.outputFiles | keys }}"  
    concurrencyLimit : 1
    tasks:
      - id: upload_to_gcs
        type: io.kestra.plugin.gcp.gcs.Upload
        from: "{{ outputs.extract_and_add_columns.outputFiles[taskrun.value] }}"
        to: "gs://{{ kv('GCP_BUCKET_NAME') }}/{{inputs.country}}/{{ taskrun.value }}"

      - id: create_bq_table_ext
        type: io.kestra.plugin.gcp.bigquery.Query
        sql: |
          CREATE OR REPLACE EXTERNAL TABLE `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.{{inputs.country}}_listings_ext` (
            id INT,
            name STRING,
            host_id INT,
            host_name STRING,
            neighbourhood_group STRING,
            neighbourhood STRING,
            latitude STRING,
            longitude STRING,
            room_type STRING,
            price FLOAT64,
            minimum_nights INT,
            number_of_reviews INT,
            last_review DATE,
            reviews_per_month FLOAT64,
            calculated_host_listings_count INT,
            availability_365 INT,
            number_of_reviews_ltm INT,
            license STRING,
            country STRING,
            region STRING,
            city STRING,
            release_date DATE
          )
          OPTIONS (
              format = 'CSV',
              uris = [ 'gs://{{ kv('GCP_BUCKET_NAME') }}/{{inputs.country}}/{{ taskrun.value }}' ],
              skip_leading_rows = 1,
              ignore_unknown_values = TRUE
          );

      - id: create_bq_table_temp
        type: io.kestra.plugin.gcp.bigquery.Query
        sql: |
          CREATE OR REPLACE TABLE `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.{{inputs.country}}_listings_temp`
          AS
          SELECT *,
            MD5(CONCAT(
              COALESCE(country, ""),
              COALESCE(region, ""),
              COALESCE(city, ""),
              COALESCE(CAST(release_date AS STRING), ""),
              COALESCE(CAST(id AS STRING), ""),
              COALESCE(CAST(host_id AS STRING), "")
            )) AS unique_row_id
          FROM `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.{{inputs.country}}_listings_ext`;

      - id: merge_bq_table
        type: io.kestra.plugin.gcp.bigquery.Query
        sql: |
          MERGE INTO `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.{{inputs.country}}_listings` T
          USING `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.{{inputs.country}}_listings_temp` S
          ON T.unique_row_id = S.unique_row_id
          WHEN NOT MATCHED THEN
            INSERT (id, name, host_id, host_name, neighbourhood_group, neighbourhood, latitude, longitude, room_type, price, minimum_nights, 
                    number_of_reviews, last_review, reviews_per_month, calculated_host_listings_count, availability_365, number_of_reviews_ltm, 
                    license, country, region, city, release_date, unique_row_id)
            VALUES (S.id, S.name, S.host_id, S.host_name, S.neighbourhood_group, S.neighbourhood, S.latitude, S.longitude, S.room_type, S.price, S.minimum_nights, 
                    S.number_of_reviews, S.last_review, S.reviews_per_month, S.calculated_host_listings_count, S.availability_365, S.number_of_reviews_ltm, 
                    S.license, S.country, S.region, S.city, S.release_date, S.unique_row_id) ;

  - id: drop_bq_tables_ext_temp
    type: io.kestra.plugin.gcp.bigquery.Query
    sql: |
      DROP TABLE IF EXISTS `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.{{inputs.country}}_listings_ext` ;
      DROP TABLE IF EXISTS `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.{{inputs.country}}_listings_temp` ;

      
  - id: purge_files
    type: io.kestra.plugin.core.storage.PurgeCurrentExecutionFiles
    description: To avoid cluttering your storage, we will remove the downloaded files

pluginDefaults:
  - type: io.kestra.plugin.gcp
    values:
      projectId: "{{kv('GCP_PROJECT_ID')}}"
      location: "{{kv('GCP_LOCATION')}}"
      bucket: "{{kv('GCP_BUCKET_NAME')}}"
      dataset: "{{kv('GCP_DATASET')}}"



triggers:
  - id: spain_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 1 1 * *"
    inputs:
      country: spain

  - id: portugal_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 2 1 * *"
    inputs:
      country: portugal

  - id: italy_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 3 1 * *"
    inputs:
      country: italy

  - id: greece_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 4 1 * *"
    inputs:
      country: greece

  - id: france_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 5 1 * *"
    inputs:
      country: france

  - id: germany_schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 6 1 * *"
    inputs:
      country: germany