id: upload_data_to_gcs
namespace: de-project

inputs:
  - id: country
    type: SELECT
    displayName: Select country
    values: [spain, portugal, italy, greece, france, germany]
    defaults: spain
    allowCustomValue: true
 
tasks:
  - id: extract_and_add_columns
    type: io.kestra.plugin.scripts.python.Script
    outputFiles:
      - "*.csv"
    taskRunner:
      type: io.kestra.plugin.core.runner.Process
    warningOnStdErr: false
    beforeCommands:
      - pip install beautifulsoup4
      - pip install wget
      - pip install pandas
      - pip install unidecode
    script: |
      import os
      import requests
      from bs4 import BeautifulSoup
      import wget
      import pandas as pd
      import unidecode
      import re

      BASE_URL = "https://insideairbnb.com/get-the-data/"

      # Fetch the webpage
      response = requests.get(BASE_URL)
      
      # Parse the HTML
      soup = BeautifulSoup(response.text, 'html.parser')

      # Find all links on the page
      all_links = soup.find_all("a")

      # Filter only "visualisations/listings.csv" links for target countries
      listings_links = [
          link.get("href") for link in all_links
          if link.get("href") and "visualisations/listings.csv" in link.get("href")
          and "{{inputs.country}}".lower() in link.get("href").lower() 
      ]
      
      download_dir = os.getcwd()
      os.makedirs(download_dir, exist_ok=True) 

      # Download each file
      for link in listings_links:
          filename_parts = link.split("/")
          country = filename_parts[3]  # Extract country from URL
          region = filename_parts[4]   # Extract region from URL
          city = filename_parts[5]     # Extract city from URL
          release_date = filename_parts[6]  # Extract release date

          # Construct a formatted filename
          file_name = f"{country}_{region}_{city}_{release_date}_listings.csv"
          file_path = os.path.join(download_dir, file_name)
          
          try:
            # Delete existing file before downloading to prevent duplicates
            if os.path.exists(file_path):
              os.remove(file_path)
            # Download file
            wget.download(link, file_path)
            print(f"\nDownloaded: {file_name}")

            # Function to clean special characters, fix encoding, and handle newlines/tabs/extra quotes
            def clean_text(text):
                if text:
                    # Remove extra newlines and tabs
                    text = re.sub(r'[\n\t]', ' ', text)
                    
                    # Remove leading/trailing quotes or extra commas
                    text = re.sub(r'^[\'",\s]+|[\'",\s]+$', '', text)
                    
                    # Normalize encoding issues using unidecode
                    text = unidecode.unidecode(text)
                    
                    # Replace multiple spaces with a single space
                    text = re.sub(r'\s+', ' ', text)
                
                return text

            # Load the CSV file into a pandas DataFrame            
            df = pd.read_csv(file_path)

            # List of all columns to clean
            columns_to_clean = [
                'id', 'name', 'host_id', 'host_name', 'neighbourhood_group', 'neighbourhood', 
                'latitude', 'longitude', 'room_type', 'price', 'minimum_nights', 'number_of_reviews', 
                'last_review', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365', 
                'number_of_reviews_ltm', 'license'
            ]

            # Apply the cleaning function to all relevant columns
            for col_name in columns_to_clean:
                if col_name in df.columns:  # Make sure the column exists in the DataFrame
                    df[col_name] = df[col_name].apply(lambda x: clean_text(str(x)) if pd.notnull(x) else x)

            # Filter rows that have exactly the expected number of columns
            df = df[df.columns[:len(columns_to_clean)].notnull().all(axis=1)]            

            # Add new columns
            df["country"] = country
            df["region"] = region
            df["city"] = city
            df["release_date"] = release_date

            # Save updated CSV
            df.to_csv(file_path, index=False)
            print(f"Updated file with additional columns: {file_name}")

          except Exception as e:
            print(f"Failed to download {file_name}: {e}")

  - id: for_each
    type: io.kestra.plugin.core.flow.ForEach
    values: "{{ outputs.extract_and_add_columns.outputFiles | keys }}"  
    tasks:
      - id: upload_to_gcs
        type: io.kestra.plugin.gcp.gcs.Upload
        from: "{{ outputs.extract_and_add_columns.outputFiles[taskrun.value] }}"
        to: "gs://{{ kv('GCP_BUCKET_NAME') }}/{{inputs.country}}/{{ taskrun.value }}"

